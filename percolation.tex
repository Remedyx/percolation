\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsthm}
\author{Marco Bertenghi}

\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{cor}{Corollary}[section]
\newtheorem{defn}{Definition}[section]
\newtheorem{exmp}{Example}[section]
\theoremstyle{definition}
\newtheorem{rem}{Remark}[section]

\title{Percolation}

\begin{document}
\maketitle
\section{Abstract}
Intuitively, percolation is a simplistic probabilistic model for a porous stone. The inside of the stone is described as a random maze in which water can flow. The question then is to understand which part of the stone will be wet when immersed in a bucket of water. The simplest and oldest model of (bond) percolation is called Bernoulli percolation and it was introduced by Broadbent and Hammersley in 1957. In this talk we will study said model and discuss some elementary results. 
\section{Framework and Definitions}
We work with a simple graph, namely the vertex set is defined as the set $\mathbb{Z}^d$ where $d \in \mathbb{N}_{ \geq 1}$ denotes the dimension and the edge set $E$ is composed of edges $e:=xy$ where $x,y \in \mathbb{Z}^d$ attain unit euclidean distance, i.e. $\|x-y\|_1 = \sum_{i=1}^d |x_i-y_i| = 1$, in percolation theory edges are called \textbf{bonds}. To simplify this exposition, we let $\mathbb{Z}^d$ refer to both, the lattice and its vertex set. 
\\\\
\textbf{Setting:} We work with the state space $\Omega=\{0,1\}^E$ where $E$ denotes the edges of our lattice $\mathbb{Z}^d$. The percolation configuration is then given by $\omega \in \{ 0,1\}^E$, i.e. $\omega(e) =1$ meaning that $e$ is open or $\omega(e)=0$ meaning that $e$ is closed. $\mathcal{F}$ is the $\sigma$-algebra generated by events depending only on finitely many edges. Finally the percolation measure is $\mathbb{P}_p=(\text{Bernoulli}(p))^{\otimes E}$ i.e. the product measure defined on all percolation configurations. 

\begin{defn} We declare each bond of the lattice $\mathbb{Z}^d$ to be \textbf{open} \textit{(water can flow)} with probability $p \in [0,1]$ and \textbf{closed} \textit{(water wont flow)} otherwise (i.e. with probability $q=1-p$). Bonds are open or closed independently of all other bonds. 
\end{defn}

\begin{defn} A \textbf{path} in $\mathbb{Z}^d$ is a sequence of vertices $(x_1, x_2, \dots , x_n)$ such that $(x_i,x_{i+1})$ is a bond of $\mathbb{Z}^d$. Such a path has \textbf{length} $n$. A path is called \textbf{open} if all its edges are open and in this case the path is said to \textbf{connect} $x_1$ with $x_n$. We say the path is \textbf{closed} if all its edges are closed. Two subgraphs of $\mathbb{Z}^d$ are \textbf{disjoint} if they have neither edges nor vertices in common.
\end{defn}
\begin{rem} In percolation we often care about open paths, because such paths simulate where water can flow. 
\end{rem}
\begin{defn} Consider the random subgraph of $\mathbb{Z}^d$ which contains only the open edges of $\mathbb{Z}^d$. The connected components (i.e. paths) of this graph are called \textbf{open clusters}. If such an open cluster is finite, it is called a \textbf{lattice animal}. We call $C(x)$ the open cluster that contains the vertex $x$. 
\end{defn}
\begin{rem}By the translation invariance of the lattice and the probability
measure $\mathbb{P}_p$, the distribution of $C(x)$ is independent of the choice of $x$. Thus we canonically choose $x=0$ to be the origin and denote $C(0)=C$ the open cluster that contains the origin. 
\end{rem}
\begin{defn} Define the percolation probability $\theta(p)$ as being the probability that the origin belongs to an infinite open cluster, formally 
\begin{align*}
\theta(p):= \mathbb{P}_p( |C| = \infty) = 1 - \sum_{n=1}^\infty \mathbb{P}_p(|C|=n)
\end{align*}
\end{defn}
\begin{rem} The event $\{ |C| = \infty \}$ can also be understood as the event that $0$ is connected to $\infty$, commonly this is denoted by $0 \longleftrightarrow \infty$. 
\end{rem}
One among the many goals of percolation theory is the study of the percolation probability which we will do next. 
\newpage 
\section{First results and the Broadbent-Hammersley Theorem}
\begin{lem} There exists a \textbf{critical value} (critical probability) $p_c=p_c(d)$ such that $\theta(p)=0$ for $p<p_c$ and $\theta(p)>0$ for $p > p_c$. Moreover, the critical value is decreasing with respect to the dimension, i.e. $p_c(d+1) \leq p_c(d)$. 
\end{lem}
\begin{proof}
Clearly, the function $p \mapsto \theta(p)$ is increasing and it holds that $\theta(0)=0$ and $\theta(1)=1$. To understand this just imagine that, heuristically speaking, as we increase the value of $p$ we increase the probability that the bonds are open, i.e. more and more water can flow and the chance that $C$ is an infinite cluster expands accordingly from $0$ to $1$. We can therefore define
\begin{align*}
p_c:= \inf \{ p \in [0,1] \mid \theta(p) > 0 \} = \sup \{ p \in [0,1] \mid \theta(p)=0 \}
\end{align*}
for said defined $p_c$ we clearly have $\theta(p)=0$ for $p<p_c$ and $\theta(p)>0$ for $p>p_c$. Moreover the graph of $\mathbb{Z}^d$ can be embedded into the graph of $\mathbb{Z}^{d'}$ for $d<d'$ by realizing $\mathbb{Z}^d$ as a linear subspace of $\mathbb{Z}^{d'}$ which is parallel to a coordinate plane. Now clearly, if the origin belongs to an infinite open cluster of $\mathbb{Z}^d$, then it is also in an infinite cluster of $\mathbb{Z}^{d'}$, this shows that $\theta(p)=\theta_d(p)$ is increasing with respect to the dimension $d$ and consequentially $p_c(d+1)\leq p_c(d) $.
\end{proof}
\begin{rem} The one-dimensional case $d=1$ is not interesting because there we have $p_c=1$. Indeed, if $p<1$ then there are infinitely many closed bonds in $\mathbb{Z}^1$ to the left and to the right of the origin, thus $\theta(p)=0$ for $p<1$ and therefore $p_c=1$. The planar case is already vastly different and for $d \geq 2$ we find the interesting results. 
\end{rem}
\begin{defn} The parameter set $p < p_c$ is called the \textbf{sub-critical phase}, the set $p>p_c$ is called the \textbf{supercritical phase}. 
\end{defn}
\begin{defn} A \textbf{self-avoiding path} of length $n$ is a sequence of edges/bonds $e_1, \dots , e_n$ with $e_i \neq e_j$ for $i \neq j$ and such that $e_i$ and $e_{i+1}$ share an endpoint for every $1 \leq i <n$. Let $\sigma(n)$ denote the number of self-avoiding paths in $\mathbb{Z}^d$ which have length $n$, we define the \textbf{connective constant} of $\mathbb{Z}^d$ as
\begin{align*}
\lambda (d) = \lim_{n \to \infty} \sigma(n)^{1/n}
\end{align*}
\end{defn}
\begin{rem} The study of self-avoiding paths is of central interest to percolation theory. The exact value of the connective constant $\lambda(d)$ is not known, only for the hexagonal lattice the value of $\lambda(d)$ is known thanks to a theorem of Duminil-Copin and Smirnow. We do however have the elementary estimate $d< \lambda(d) < 2d-1$, indeed a walk going only forward in each direction is self avoiding and a self-avoiding path can not reverse direction and so $\sigma(n) \leq 2d(2d-1)^{n-1}$ (show by induction over $n \in \mathbb{N}$). 
\end{rem}
\newpage
\begin{defn} Let $G$ be a graph, we define its dual $G^*$ as the graph which has as vertices the faces of $G$ and as vertices pairs of faces which are adjacent. 
\end{defn}
\begin{thm}[Broadbent-Hammersley Theorem] For $d\geq 2$ we have 
\begin{align*}
0 < \lambda(d)^{-1} \leq p_c(d) \leq p_c(2) < 1.
\end{align*}
\end{thm}
\begin{proof}
We will first establish that $p_c(d) \geq \lambda(d)^{-1}$. \\\\
Let $N(n)$ denote the number of \textit{open} self-avoiding paths of length $n$ in $\mathbb{Z}^d$. Since any such path is open with probability $p^n$ we have 
\begin{align*}
\mathbb{E}_p(N(n))=p^n \sigma(n). 
\end{align*}
If the origin is in an infinite open cluster, then there must exist open paths of arbitrary lengths beginning at the origin, using this and Markov's inequality we obtain 
\begin{align*}
\theta(p) \leq \mathbb{P}_p(N(n) \geq 1) \leq \mathbb{E}_p(N(n)) = p^n \sigma(n) \overset{\text{Def.}} = ( p  \lambda(d) + o(1))^n 
\end{align*}
the above goes to zero for $p < \lambda(d)^{-1}$. By definition of the critical probability $p_c$ this shows that $p_c(d) \geq \lambda(d)^{-1}$.
\\\\
Next we show that $p_c(2) < 1$. That is, we need to verify that if $p$ is close to $1$, then $\theta(p) >0$. 
\\\\
Let $\mathbb{Z}_*^2$ denote the dual graph of $\mathbb{Z}^2$, in this particular case this just means that $\mathbb{Z}_*^2$ is $\mathbb{Z}^2$ translated by the vector $(1/2,1/2)$. On $\mathbb{Z}_*^2$ we define the \textbf{dual (bond) configuration} as follows. Every edge $e$ of $\mathbb{Z}^2$ is naturally associated with exactly one edge $e^*$ of $\mathbb{Z}_*^2$ intersecting it in its middle. We thus say that $e^*$ is open if it crosses an open edge in $\mathbb{Z}^2$.  
\\\\
By the \textbf{Jordan curve theorem} we get that the open cluster containing the origin is finite if and only if the origin is in the interior of a closed circuit (i.e. a self-avoiding path $x_0 \sim \dots \sim x_k$ with $x_0=x_k$ consisting only of closed bonds) of the dual lattice.  
\\\\
Let $\rho(n)$ denote the number of closed circuits in $\mathbb{Z}_*^2$ which have length $n$ and contain in their interiors the origin of $\mathbb{Z}^2$. Each such a circuit contains a self-avoiding path of length $n-1$ starting from a vertex of the form $(k+1/2,1/2)$ where $0 \leq k <n$. Let $\gamma$ denote any such path, then there can be at most $n \sigma(n-1)$ of them and thus $\rho(n) \leq n \sigma(n-1)$. 
\newpage 
Following the same line of reasoning as we did in the first part of the proof with $q=1-p$ we obtain that 
\begin{align*}
\sum_{ \gamma } \mathbb{P}_p( \gamma \text{ is closed}) \leq \sum_{n=1}^\infty q^n n \sigma(n-1) = \sum_{n=1}^\infty qn( q \lambda(2) + o(1))^{n-1}.
\end{align*}
Using the ratio test we easily establish that the rightmost sum above converges if and only if $q \lambda(2) <1$. Furthermore, this sum goes to zero if $q \to 0$ so we can always find $0 < \delta <1$ such that $p> \delta$ we have 
\begin{align*}
\sum_{ \gamma } \mathbb{P}_p( \gamma \text{ is closed} ) \leq 1/2 
\end{align*}
Thus (by the equivalence assured via the Jordan Curve Theorem) we get 
\begin{align*}
\theta(p) = \mathbb{P}_p(|C|= \infty) = \mathbb{P}_p(\text{no $\gamma$ is closed}) \geq 1 - \sum_{\gamma} \mathbb{P}_p( \gamma \text{ is closed} ) \geq 1/2 >0,
\end{align*}
thus we have shown that $p_c(2) \leq \delta <1$. 
\end{proof}
\begin{rem} The centerpiece of this theorem is that in two or more dimension there are two phases of the process. In the subcritical phase, i.e. when $p < p_c(d)$ we have $\theta(p)=0$ and the origin is almost surely in a finite open cluster. Contrary to this, in the supercritical phase, i.e. when $p>p_c$, the origin has a strictly positive probability of being in an infinite open cluster. Moreover the theorem establishes that for $d \geq 2$ the critical probability is not trivial i.e. $p_c(d) \notin \lbrace 0,1 \rbrace$. 
\\\\
It is conjectured that $\theta(p_c) = 0$ for all $d \geq 3$, but at the moment, it is known rigorously only for the case $d=2$ (Kesten) and for $d$ large, i.e. $d \geq 19$. The case $d \geq 19$ has been treated by Hara and Slade (1990) with a technique known under the name of \textit{lace expansion}. Each few years, more delicate uses of the lace-expansion enable mathematicians to prove the conjecture for lower dimensions of $d$. Currently, the state of the art is at $d \geq 11$. It is expected that the technique of lace expansion wont give any results for $d \leq 5$. This leaves the intermediate dimensions $3,4$ and $5$ as a beautiful challenge to mathematicians. In particular the physically relevant case $d=3$ is still an open problem. 
\end{rem}
\newpage
\section{Harris-FKG inequality and improvement of the critical bound $p_c(2)$}
Recall that we work on the state space $\Omega = \{0,1\}^E$, there exists a natural partial ordering in $\Omega$ coming from the ordering on $\{0,1\}$.
\begin{defn} Let $\omega, \omega'$ be two configurations/realisations of Bernoulli percolations. We say $\omega \leq \omega'$ if $\omega(e) \leq \omega'(e)$ for all bonds $e \in \mathbb{Z}^d$. We say a random variable $X \colon \Omega= \{0,1\}^E \to \mathbb{R}$ is \textbf{increasing} if for $\omega \leq \omega'$ we have $X(\omega) \leq X(\omega')$. We say $X$ is \textbf{decreasing} if $-X$ is increasing. Finally we can say that an event $A \in \mathcal{A}$ is increasing if $1_A$ is increasing. 
\end{defn}
\begin{rem} Intuitively, $A$ being an increasing event means that it is favored by opening up edges, i.e. if $\omega \in A$ ($A$ is satisfied) and $\omega \leq \omega'$ (we choose some closed edges in $\omega$ and open them up), then $\omega' \in A$ ($A$ is still satisfied). 
\end{rem}
\begin{exmp} With our definitions fixed above we can say that the event $\{|C|= \infty\}$ (there exists an infinite open cluster) that $0$ (or more generally $x$) is connected to $\infty$ is an increasing event. 
\end{exmp}
We will next introduce a construction which is fundamental for many proofs in percolation theory called coupling.\\\\
\textbf{Standard coupling of percolation:} Let us consider (on a much bigger probability space $\Omega'$) a family of i.i.d. random variables $(X_e)_{e \in E}$, where each $X_e$ is uniformly distributed on $[0,1]$. In other words, we put a random label between $0$ and $1$ on each edge of our percolation model, independently. Then for all $p \in [0,1]$, we can define the configuration $\omega_p=( \omega_p(e))_{e \in E}$ by 
\begin{align*}
\omega_p(e) = 1_{X_e \leq p}, \text{ for all } e \in E
\end{align*}
\begin{prop}[increasing coupling] Fix $p \leq p'$. There exists a probability measure $\mathbb{P}$ on $[0,1]^E $ which coincides with $\mathbb{P}_p$ on $\{0,1\}^E$, such that 
\begin{align*}
\mathbb{P}( \omega_p \leq \omega_{p'} ) = 1
\end{align*}
\end{prop}
\begin{proof}
Following the construction above, it is easy to check that the law of $\omega_p$ is $\mathbb{P}_p$. Indeed, each edge is open with probability $p$ and closed with probability $1-p$ independently of the state of the other edges. By construction we have $\omega_p \leq \omega_{p'}$. Thus we only need to define $\mathbb{P}$ to be the law of $(X_e)_{e \in E}$.
\end{proof}
\begin{lem} If $X$ is an increasing random variable in $L^1( \Omega, \mathbb{P}_q) \cap  L^1( \Omega, \mathbb{P}_p)$ then we have 
\begin{align*}
\mathbb{E}_p(X) \leq \mathbb{E}_q(X), \text{ for } p \leq q.
\end{align*}
\end{lem}
\begin{proof}
We simply have to apply the increasing coupling technique that we introduced. Let $(U_e)_{e \in E}$ be a sequence of i.i.d. uniform random variables on $[0,1]$. For every $p \in [0,1]$ we define $\omega_p(e) = 1_{U_e \leq p}$. Thanks to increasing coupling we know that for $p \leq q$ we have $\omega_p \leq \omega_{q}$ almost surely. Since $X$ is an increasing random variable we have by definition $X( \omega_p) \leq X(\omega_{q})$ almost surely. Since $\mathbb{P}$ has coincides $\mathbb{P}_p$ i.e. $\mathbb{E}(X( \omega_p))= \mathbb{E}_p(X)$ we obtain
\begin{align*}
\mathbb{E}_p(X) = \mathbb{E}(X( \omega_p)) \leq \mathbb{E}(X( \omega_{q})) = \mathbb{E}_{q} (X) 
\end{align*}
\end{proof}
%\begin{proof}
%If $X$ depends only on a single bond $e$, we can write $\mathbb{E}_p(X)= pX(1)+(1-p)X(0)$. Because $X$ is assumed to be increasing, we get after differentiating with respect to $p$ 
%\begin{align*}
%\frac{d}{dp} \mathbb{E}_p(X)= X(1) - X(0) \geq 0
%\end{align*}
%which means that $p \mapsto \mathbb{E}_p$ is increasing, i.e. $\mathbb{E}_p(X) \leq \mathbb{E}_q(X)$ whenever $p \leq q$. If $X$ depends only on finitely many bonds,  we can write it as a sum $X= \sum_{i=1}^d X_i$ of variables $X_i$ which depend only on one bond and we get again 
%\begin{align*}
%\frac{d}{dp} \mathbb{E}_p(X) = \sum_{i=1}^n (X_i(1)-X_i(0)) \geq 0.
%\end{align*}
%In general, we can approximate every random variable in $L^1( \Omega, \mathbb{P}_q) \cap  L^1( \Omega, \mathbb{P}_p)$ by step functions which depend only on finitely many coordinates $X_i$. Since we have $\mathbb{E}_p(X_i) \to \mathbb{E}_p(X)$ and $\mathbb{E}_q(X_i) \to \mathbb{E}_q(X)$ as $i \to \infty$ the claim follows. 
%\end{proof}
\begin{rem} We know that the event $A= \{ |C| = \infty\}$ is increasing, in particular the random variable $1_A$ is increasing, thus by the above Lemma we get a rigorous proof of the statement that $\theta(p)= \mathbb{P}_p( |C| = \infty)$ is increasing. 
\end{rem}
To estimate or compute the probability of events, it is often necessary to compute the probability of intersections. Unfortunately,  when two events are not independent, it is rarely possible to compute the probability exactly. In the case of percolation, it is nonetheless possible to bound this probability for a large class of events. The following inequality is named after Fortuin, Kasterleyn and Ginibre, these authors have however proved a similar inequality in a more general context, therefore the name Harris-FKG. 
\begin{thm}[FKG inequality] For increasing random variables $X,Y$ in $L^2( \Omega, \mathbb{P}_p)$, we have 
\begin{align*}
\mathbb{E}_p(XY) \geq \mathbb{E}_p(X) \cdot  \mathbb{E}_p(Y).
\end{align*}
\end{thm}
\begin{proof}
We will first prove the claim for random variables $X,Y$ which depend only on $n$ edges $e_1, e_2, \dots , e_n$ and proceed by induction over $n \in \mathbb{N}$. To establish the theorem in its full generality we use a limiting technique. We therefore segment this proof into a finite volume and an infinite volume case. Let us denote $\omega_i = \omega(e_i)$.
\newpage 
i) \textbf{Finite volume:} Let $f,g : \{0,1\}^n \to \mathbb{R}$ be increasing, for $p \in [0,1]$ we claim \begin{align*}
(\mathcal{P}_n) : \ \mathbb{E}_p( f ( \omega_1, \dots , \omega_n) g ( \omega_1, \dots , \omega_n)) \geq \mathbb{E}_p(f( \omega_1, \dots , \omega_n)) \mathbb{E}_p( g ( \omega_1, \dots , \omega_n)) 
\end{align*}
\textbf{n=1:} Let $f,g : \{0,1\} \to \mathbb{R}$ be increasing. Without loss of generality we can assume that $f(0)=g(0)=0$ because adding a constant to $f$ and/or $g$ will not change the inequality we want to establish in $(\mathcal{P}_n)$. Since $f,g$ are increasing we then must have that $f(1) \geq 0, g(1) \geq 0$. 
\begin{align*}
&\mathbb{E}_p(f(\omega_1) g(\omega_1)) - \mathbb{E}_p( f(\omega_1))\mathbb{E}_p(g ( \omega_1))\\  &= p f(1)g(1) + (1-p)f(0)g(0) -(pf(1) + (1-p)f(0))(pg(1)+(1-p)g(0)) \\
&= pf(1)g(1)-p^2f(1)g(1) \geq 0 \text{ (since $p^2 \leq p$ on $[0,1]$})
\end{align*}
Let now $n \geq 1$ and assume that $( \mathcal{P}_n)$ holds. Take $f,g : \{0,1\}^{n+1} \to \mathbb{R}$ increasing functions (depending on $\omega_1, \dots , \omega_{n+1})$. By conditioning on $\omega_1, \dots , \omega_n$ we obtain by the definition of conditional expectation that 
\begin{align*}
\mathbb{E}_p(fg) &= \mathbb{E}_p(\mathbb{E}_p( fg \mid \omega_1, \dots , \omega_n))
\\
& \overset{(\mathcal{P}_1)}\geq \mathbb{E}_p( \mathbb{E}_p(f \mid \omega_1, \dots , \omega_n) \cdot \mathbb{E}_p( g \mid \omega_1,  \dots , \omega_n)),
\end{align*}
by using the case of one single variable $\omega_{n+1}$ (with $\omega_1, \dots , \omega_n$ fixed). Then, we apply the induction hypothesis $(\mathcal{P}_n)$ to the functions $\mathbb{E}_p(f \mid \omega_1, \dots , \omega_n))$ and $\mathbb{E}_p(g \mid \omega_1, \dots , \omega_n))$ for which we can easily verify that they are both still increasing functions of $\omega_1, \dots , \omega_n$. 
In that manner we obtain 
\begin{align*}
\mathbb{E}_p(fg) &= \mathbb{E}_p(\mathbb{E}_p( fg \mid \omega_1, \dots , \omega_n))
\\
& \overset{(\mathcal{P}_1)}\geq \mathbb{E}_p( \mathbb{E}_p(f \mid \omega_1, \dots , \omega_n) \cdot \mathbb{E}_p( g \mid \omega_1,  \dots , \omega_n))\\
& \overset{( \mathcal{P}_n)} \geq \mathbb{E}_p( \mathbb{E}_p(f \mid \omega_1, \dots ,  \omega_n)) \cdot \mathbb{E}_p( \mathbb{E}_p ( g \mid \omega_1, \dots , \omega_n)) \\
& = \mathbb{E}_p(f) \mathbb{E}_p(g)
\end{align*}
Thus we have shown that $(\mathcal{P}_n) \rightsquigarrow ( \mathcal{P}_{n+1})$, the result follows thus by induction on $n \in \mathbb{N}$. 
\newpage
ii) \textbf{Infinite volume:} In this step we lift the restriction that $X$ and $Y$ depend on finitely many edges only. Let $\mathcal{A}_n$ denote the $\sigma$-algebra generated by $( \omega_i)_{1 \leq i \leq n}$.
\\
\\
 Let $X,Y \in L^2( \Omega, \mathbb{P}_p)$ be arbitrary increasing random variables. Let us define $X_n = \mathbb{E}_p(X \mid \mathcal{A}_n)$ and $Y_n = \mathbb{E}_p(Y \mid \mathcal{A}_n)$. It is then easy to verify that $X_n, Y_n$ are still increasing in $(\omega_1, \dots, \omega_n)$. By our efforts in the first part of the proof we know that $\mathbb{E}_p(X_nY_n) \geq \mathbb{E}_p(X_n) \mathbb{E}_p(Y_n)$. Since $(X_n)_{n \in \mathbb{N}}$ and $(Y_n)_{n \in \mathbb{N}}$ are martingales which are both bounded in $L^2( \Omega, \mathbb{P}_p)$ we know that they must converge in $L^2$, in fact we have $X_n \to X$ and $Y_n \to Y$ in $L^2$ and consequently $\mathbb{E}_p(X_n) \to \mathbb{E}_p(X)$ and $\mathbb{E}_p(Y_n) \to \mathbb{E}_p(Y)$. By Hölder's inequality we obtain
\begin{align*}
\|X_nY_n - XY\|_1 & \leq \|(X_n-X)Y_n\|_1 + \|X(Y_n-Y)\|_1 
\\ & \leq \| X_n-X\|_2 \|Y_n\|_2 + \|X\|_2 \|Y_n-Y\|_2 \to 0.
\end{align*}
This means that $\mathbb{E}_p(X_nY_n) \to \mathbb{E}_p(XY)$ as $n \to \infty$. Bringing it all together we have shown that
\begin{align*}
\mathbb{E}_p(X) \mathbb{E}_p(Y) = \mathbb{E}_p(X_n)\mathbb{E}_p(Y_n) \leq \mathbb{E}_p(X_nY_n) \to \mathbb{E}_p(XY).
\end{align*}
\end{proof}
\begin{rem} It follows immediately from the FKG inequality that if $A,B$ are increasing events in $\Omega$, then $\mathbb{P}_p(A \cap B) \geq \mathbb{P}_p(A) \mathbb{P}_p(B)$ which can also be restated as $\mathbb{P}_p(A \mid B) \geq \mathbb{P}_p(A)$. Intuitively this means that two increasing events $A$ and $B$ are positively correlated. 
\end{rem}
\begin{exmp}
Let $(\Gamma_i)_{i=1}^k$ be families of paths in $\mathbb{Z}^d$ and let $A_i$ be the event that some path in $\Gamma_i$ is open. Then the events $A_i$ are increasing, since also the intersection of increasing events is increasing we obtain after applying the FKG-inequality $k$-times that 
\begin{align*}
\mathbb{P}_p \left( \bigcap_{i=1}^k A_i \right) \geq \prod_{i=1}^k \mathbb{P}_p(A_i)
\end{align*}
\end{exmp}
We show now, how this inequality can be used to give an explicit bound for the critical percolation probability $p_c(2)$ in $\mathbb{Z}^2$. 
\newpage
\begin{cor} \begin{align*}
p_c(2) \leq (1- \lambda(2)^{-1}). 
\end{align*}
\end{cor}
\begin{proof}
Let $N \in \mathbb{N}$, we define the events
\begin{align*}
F_N &= \{ \exists \text{ no closed path of length} \leq N \text{ in } \mathbb{Z}_*^d \} 
\\
G_N & = \{ \exists \text{ no closed path of length} > N \text{ in } \mathbb{Z}_*^d \} 
\end{align*}
We already know that $F_N \cap G_N = \{|C| = \infty \}$. Since both $F_N$ and $G_N$ are increasing, the FKG correlation inequality says that $\mathbb{P}_p( F_N \cap G_N) \geq \mathbb{P}_p(F_N) \mathbb{P}_p(G_N)$, or in other words we have 
\begin{align*}
\theta(p) = \mathbb{P}_p(|C| = \infty ) = \mathbb{P}_p( F_N \cap G_N) \geq \mathbb{P}_p(F_N) \mathbb{P}_p(G_N).
\end{align*}
We have already seen that if $(1-p) \lambda(2) < 1$, then we have
\begin{align*}
\mathbb{P}_p(G_N^c) \leq \sum_{n=N}^\infty (1-p)^n n \sigma(n-1) < \infty
\end{align*}
which goes to zero for $N \to \infty$. Thus, for $N$ large enough, we have $\mathbb{P}_p(G_N) \geq 1/2$. Moreover, we do also have that $\mathbb{P}_p(F_N)>0$ for $N$ large enough. It follows that $\theta_p >0$ if $(1-p) \lambda(2) <1$ or equivalently $p < (1- \lambda(2)^{-1})$ which entails that $p_c(2) \leq (1- \lambda(2)^{-1})$. 
\end{proof}
\newpage
\section{Pivotal edges and Russo's formula}
\begin{defn} Let $A \in \mathcal{A}$ be an event and $\omega \in \Omega$ a configuration. We say that an edge $e \in \mathbb{Z}^d$ is \textbf{pivotal} for the pair $(A, \omega)$ if $1_A(\omega) \neq 1_A( \widetilde{\omega_e})$ where $\widetilde{\omega_e}$ is the unique configuration which agrees with $\omega$ except at the edge $e$. We also define random variables 
\begin{align*}
\omega_e := \begin{cases} \omega_e(e)=0 \\ \omega_e(f)= \omega(f), \ f \neq e  \end{cases}, \  \ \ \omega^e := \begin{cases} \omega^e(e)=1 \\ \omega^e(f) = \omega(f), \ f \neq e \end{cases}
\end{align*}
\end{defn}
\begin{rem} The definition above suggests that $e$ is pivotal for $(A, \omega)$ if the occurrence or non occurrence of the event $A$ depends crucially on the state of $e$. In other words, pivotal edges are the edges that are essential for an event $A$ to be satisfied. 
\end{rem}
\begin{exmp} Let $A= \{ |C| = \infty\}$ be the event that the origin is in an infinite open cluster. The edge $e$ is pivotal for $A$ if, when $e$ is removed from the lattice, one endvertex of $e$ is in a finite open cluster which includes the origin, and the other endvertex of $e$ is in an infinite open cluster (not containing the origin).
\end{exmp}
\begin{thm}[Russo's formula] Let $A$ be an increasing event depending only on finitely many edges of $\mathbb{Z}^d$. Then $p \mapsto \mathbb{P}_p(A)$ is differentiable, and 
\begin{align*}
\frac{d}{dp} \mathbb{P}_p(A) = \sum_{e \in F} \mathbb{P}_p (e \text{ is pivotal for }A) = \mathbb{E}_p(N(A)),
\end{align*}
where $F \subset E$ is a finite subset and $N(A)$ denotes the number of edges which are pivotal for $A$. 
\end{thm}
\begin{rem} In words, Russo's formula means that the derivative of the probability of an increasing event is equal to the expected number of pivotal points for said event. Russo's formula shows that pivotal edges determine the rate of variation of $\mathbb{P}_p(A)$. 
\end{rem}
\begin{proof} \textit{(Russo's Formula)} The main idea is to consider parameters $(p_e)_{e \in F}$ varying independently, instead restricting ourselves to $p_e =p$ for all $e \in E$. We thus introduce the function
\begin{align*}
\mathcal{P} \colon \overline{p} = (p_e)_{e \in F} \mapsto \mathbb{P}_{\overline{p}}(A),
\end{align*}
where $\mathbb{P}_{\overline{p}} = \prod_{e \in F}\text{Bernoulli}(p_e)$ is our product measure. 
We fix a ordering of $F$, we write $F= \{e_1, \dots , e_n\}$. 
\\
\\
First, we can note that $\mathcal{P}$ is a polynomial function in $p_{e_1}, \dots , p_{e_n}$ so in particular a $C^\infty$ function, its differentiability is thus clear. Let us fix $i \in \{1, \dots , n\}$, and compute $\frac{\partial \mathcal{P}}{\partial p_{e_i}}$. We consider some $\overline{p}$, and $\overline{p}'$ that coincides with $\overline{p}$ except at $e_i$. We can without loss of generality assume that $p'_{e_i} \geq p_{e_i}$ (the same reasoning would apply in the other case). 
\\\\
We now use the standard coupling: we denote by $\mathbb{P}$ the corresponding probability measure (as in the Proposition), and by $(X_e)_{e \in E}$ the associated random variables (marking on the edges). We have
\begin{align*}
\mathcal{P}(\overline{p}') - \mathcal{P}(\overline{p}) &= \mathbb{P}( \omega_{\overline{p}'} \in A ) - \mathbb{P}( \omega_{\overline{p}} \in A) \\
&= \mathbb{P}( \omega_{\overline{p}'} \in A, \ \omega_{\overline{p}} \notin A)
\end{align*}
using that $\{ \omega_{\overline{p}} \in A \} \subset \{ \omega_{\overline{p}'} \in A \}$, this follows from the fact that $\omega_{\overline{p}} \leq \omega_{\overline{p}'}$ (since $\overline{p} \leq \overline{p}'$), and the assumption that $A$ is increasing. We then verify the following identity: 
\begin{align*}
\mathbb{P}( \omega_{\overline{p}'} \in A, \ \omega_{\overline{p}} \notin A) = \mathbb{P}( p_{e_i} < X_{e_i} \leq p_{e_i}', \ e_i \text{ is pivotal for } (A, \omega_{\overline{p}})). 
\end{align*}
Indeed, let us assume that $\{\omega_{\overline{p}'} \in A, \ \omega_{\overline{p}} \notin A\}$ occurs, then  
\begin{enumerate}
\item The two configurations $\omega_{\overline{p}}$ and $\omega_{\overline{p}'}$ coincide, except possibly at $e_i$: the only possible difference is that $e_i$ is closed in  $\omega_{\overline{p}}$ and open in $\omega_{\overline{p}'}$ (because $\omega_{\overline{p}} \leq \omega_{\overline{p}'}$) but this occurs \textit{iff} $p_{e_i} < X_{e_i} \leq p_{e_i}'$ (by definition of the standard coupling). 
\item If they do differ at $e_i$, then $\{\omega_{\overline{p}'} \in A, \ \omega_{\overline{p}} \notin A\}$ occurs \textit{iff} $e_i$ is pivotal for $(A, \omega_{\overline{p}})$. 
\end{enumerate}
Note that for every $e \in E$, the event $\{e \text{ is pivotal for } A\}$ is the set of configurations $\omega$ for which $e$ is pivotal
for $(A, \omega)$. Clearly this event depends only on the states of edges
other than $e$; it is independent of the state of $e$ itself. In particular we obtain that $\{e \text{ is pivotal for }A\}$ is, by definition, independent of the random variable $\omega_e$. Consequentially, the two events $\{p_{e_i} < X_{e_i} \leq p_{e_i}'\}$ and $\{e_i$ is pivotal for $(A, \omega_{\overline{p}})\}$ are independent, which yields 
\begin{align*}
\mathcal{P}(\overline{p}') - \mathcal{P}(\overline{p}) &= \mathbb{P}(p_{e_i} < X_{e_i} \leq p_{e_i}' ) \mathbb{P}_{\overline{p}} (e_i \text{ is pivotal for }A) \\
&= (p_{e_i}'-p_{e_i}) \mathbb{P}_{\overline{p}} (e_i \text{ is pivotal for }A),
\end{align*}
because $X_{e_i}$ is uniformly distributed on $[0,1]$. Hence, 
\begin{align*}
\frac{\partial \mathcal{P}}{\partial p_{e_i}}( \overline{p}) = \mathbb{P}_{\overline{p}} ( e_i \text{ is pivotal for }A). 
\end{align*}
Russo's formula follows now by using the chain rule: since $\mathbb{P}_p(A)= \mathcal{P}(p, \dots , p)$, we have 
\begin{align*}
\frac{d}{dp} \mathbb{P}_p(A) = \sum_{i=1}^n \frac{\partial P}{\partial p_{e_i}}(p, \dots , p) = \sum_{i=1}^n \mathbb{P}_p(e_i \text{ is pivotal for }A). 
\end{align*}
\end{proof}
\section{Application of Russo's formula: Exponential decay property}
One major application of Russo's formula is to prove that in the subcritical regime (i.e. when $p < p_c)$, the connection probabilities decay exponentially fast with distance. This result has first been proven by Meshikov in 1986. Let $x,y,$ be two vertices in $\mathbb{Z}^d$ for $d \geq 2$, then $\mathbb{P}_p( x \leftrightarrow y ) \to 0$ exponentially fast as $\|x - y \|_1 \to \infty$. \\
\\
The proof of this result is quite technical and way beyond the scope of this talk, it is however worth mentioning that one of the more elegant proofs of said theorem was proposed quite recently (in 2014 to be precise) by Hugo Duminil-Copin and Vincent Tassion, in fact they have provided a slightly stronger result: 
\begin{thm}[Exponential decay] For all $d \geq 2$, we have in the subcritical regime $p < p_c$, that there exists a constant $c=c(p) >0$ such that for all $n \geq 1$ 
\begin{align*}
\mathbb{P}_p( 0 \leftrightarrow \partial B_n)  \leq e^{-cn},
\end{align*}
where $B_n:= \{-n, \dots , n\}^d$ denotes the box of size $n$ around the origin.
\end{thm}
The result above can also be understood in a more picturesque manner; in the subcritical regime, the mean size of the open clusters are finite, in fact with an increase of distance they rapidly (exponentially) thin out. By duality, using $q=1-p > p_c$ we can say that there exists one open cluster of infinite size. 
\end{document}
